{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOKrhsR6ZYOEGCJJqrYKfGG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArayaTripathi/Gingivitis-detection-model/blob/main/Gingivitis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "!pip install torch torchvision scikit-learn matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BscRCm5AUvAs",
        "outputId": "5ae361ba-4420-48b6-d18a-59a08254f734"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.2.9-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2025.8.3)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (11.3.0)\n",
            "Collecting pi-heif<2 (from roboflow)\n",
            "  Downloading pi_heif-1.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow)\n",
            "  Downloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (4.59.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (3.2.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow) (3.4.3)\n",
            "Downloading roboflow-1.2.9-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.7/88.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.0 pillow-avif-plugin-1.5.2 roboflow-1.2.9\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow\n",
        "\n",
        "rf = Roboflow(api_key=\"uwwTsnHAguK00gGGxInO\")\n",
        "project = rf.workspace(\"betic-project\").project(\"new-gingivitis-classification-ojpsc\")\n",
        "version = project.version(3)\n",
        "dataset = version.download(\"folder\")  # downloads to /content/folder\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMiFNmuRU163",
        "outputId": "8c6fd1cd-d467-4126-a591-e4c5da29f24b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in New-Gingivitis-classification-3 to folder:: 100%|██████████| 1318/1318 [00:00<00:00, 1907.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to New-Gingivitis-classification-3 in folder:: 100%|██████████| 303/303 [00:00<00:00, 6871.82it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set paths\n",
        "data_dir = os.path.join(\"folder\")  # or dataset.location\n",
        "train_dir = os.path.join(data_dir, \"/content/New-Gingivitis-classification-3/train\")\n",
        "valid_dir = os.path.join(data_dir, \"/content/New-Gingivitis-classification-3/valid\")\n",
        "\n",
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Datasets\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
        "valid_dataset = datasets.ImageFolder(valid_dir, transform=transform)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "class_names = train_dataset.classes\n"
      ],
      "metadata": {
        "id": "UGeKzpArU6XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load ResNet-34\n",
        "model = models.resnet34(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, len(class_names))  # Adjust output layer\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2IVtz07U-5U",
        "outputId": "5d26da81-344d-4f0c-ba49-2dfad29518fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 188MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 50  # increase for better performance\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aogz_aJVCmN",
        "outputId": "a46d4da6-abc9-4d74-fd13-91ea2f0a0546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 1.9631608426570892\n",
            "Epoch 2/50, Loss: 0.9597252309322357\n",
            "Epoch 3/50, Loss: 0.8170333877205849\n",
            "Epoch 4/50, Loss: 0.5646812561899424\n",
            "Epoch 5/50, Loss: 0.437220711261034\n",
            "Epoch 6/50, Loss: 0.4972009211778641\n",
            "Epoch 7/50, Loss: 0.26440953090786934\n",
            "Epoch 8/50, Loss: 0.5121491495519876\n",
            "Epoch 9/50, Loss: 0.28941552294418216\n",
            "Epoch 10/50, Loss: 0.35561258159577847\n",
            "Epoch 11/50, Loss: 0.5579711254686117\n",
            "Epoch 12/50, Loss: 0.6165427453815937\n",
            "Epoch 13/50, Loss: 0.6390258967876434\n",
            "Epoch 14/50, Loss: 0.5615229830145836\n",
            "Epoch 15/50, Loss: 0.5709259286522865\n",
            "Epoch 16/50, Loss: 0.7194061912596226\n",
            "Epoch 17/50, Loss: 0.5540519077330828\n",
            "Epoch 18/50, Loss: 0.48469293117523193\n",
            "Epoch 19/50, Loss: 0.2686875695362687\n",
            "Epoch 20/50, Loss: 0.18527246871963143\n",
            "Epoch 21/50, Loss: 0.1087113672401756\n",
            "Epoch 22/50, Loss: 0.11210769368335605\n",
            "Epoch 23/50, Loss: 0.07401996990665793\n",
            "Epoch 24/50, Loss: 0.0660010795108974\n",
            "Epoch 25/50, Loss: 0.06326125195482746\n",
            "Epoch 26/50, Loss: 0.07618417125195265\n",
            "Epoch 27/50, Loss: 0.309895277954638\n",
            "Epoch 28/50, Loss: 0.33639928326010704\n",
            "Epoch 29/50, Loss: 0.35996093042194843\n",
            "Epoch 30/50, Loss: 0.5125301666557789\n",
            "Epoch 31/50, Loss: 0.4944817740470171\n",
            "Epoch 32/50, Loss: 0.39509127475321293\n",
            "Epoch 33/50, Loss: 0.3018752969801426\n",
            "Epoch 34/50, Loss: 0.24977577570825815\n",
            "Epoch 35/50, Loss: 0.2818620214238763\n",
            "Epoch 36/50, Loss: 0.19438119512051344\n",
            "Epoch 37/50, Loss: 0.23748290911316872\n",
            "Epoch 38/50, Loss: 0.4397070021368563\n",
            "Epoch 39/50, Loss: 0.40409617125988007\n",
            "Epoch 40/50, Loss: 0.3349467143416405\n",
            "Epoch 41/50, Loss: 0.3722765166312456\n",
            "Epoch 42/50, Loss: 0.3757962165400386\n",
            "Epoch 43/50, Loss: 0.3015970936976373\n",
            "Epoch 44/50, Loss: 0.2557082334533334\n",
            "Epoch 45/50, Loss: 0.18169395718723536\n",
            "Epoch 46/50, Loss: 0.10254476114641875\n",
            "Epoch 47/50, Loss: 0.06832579872570932\n",
            "Epoch 48/50, Loss: 0.04826097027398646\n",
            "Epoch 49/50, Loss: 0.13301879214122891\n",
            "Epoch 50/50, Loss: 0.11236420599743724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in valid_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        y_true.extend(labels.numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Sensitivity (Recall) & Specificity (for binary or per class)\n",
        "report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "for cls in class_names:\n",
        "    print(f\"{cls} - Sensitivity (Recall): {report[cls]['recall']:.4f}, Specificity: TBD\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLA_qXICVIB2",
        "outputId": "46f76939-a06a-46e0-8704-1c3b508e72ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.3929\n",
            "Mild - Sensitivity (Recall): 0.7500, Specificity: TBD\n",
            "Normal - Sensitivity (Recall): 0.4000, Specificity: TBD\n",
            "moderate - Sensitivity (Recall): 0.0000, Specificity: TBD\n",
            "severe - Sensitivity (Recall): 0.0000, Specificity: TBD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For binary classification:\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "print(f\"Sensitivity: {sensitivity:.4f}\")\n",
        "print(f\"Specificity: {specificity:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "D4OK-bFsVWof",
        "outputId": "1494785a-bc58-45cd-8cf4-bc722496e354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 4)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4086648106.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# For binary classification:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msensitivity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mspecificity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtn\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"resnet34_gingivitis.pth\")\n"
      ],
      "metadata": {
        "id": "u6aDEkDKVYnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2a9e129"
      },
      "source": [
        "# Task\n",
        "Improve the accuracy of the image classification model to 89.3% or higher."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8bfbe99"
      },
      "source": [
        "## Increase epochs\n",
        "\n",
        "### Subtask:\n",
        "Train the model for more epochs to allow it to learn more from the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b13ebab2"
      },
      "source": [
        "**Reasoning**:\n",
        "Increase the number of training epochs and rerun the training loop to improve model performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3f01387",
        "outputId": "5b200edc-7be3-48b2-c67a-c4cebe6b92e9"
      },
      "source": [
        "num_epochs = 100  # increased for better performance\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 0.573186720488593\n",
            "Epoch 2/100, Loss: 0.24257121235132217\n",
            "Epoch 3/100, Loss: 0.45592578314244747\n",
            "Epoch 4/100, Loss: 0.33081923332065344\n",
            "Epoch 5/100, Loss: 0.15837876894511282\n",
            "Epoch 6/100, Loss: 0.1655716998502612\n",
            "Epoch 7/100, Loss: 0.10169835295528173\n",
            "Epoch 8/100, Loss: 0.05799546022899449\n",
            "Epoch 9/100, Loss: 0.04922559589613229\n",
            "Epoch 10/100, Loss: 0.09207282646093518\n",
            "Epoch 11/100, Loss: 0.07150943111628294\n",
            "Epoch 12/100, Loss: 0.30699684913270175\n",
            "Epoch 13/100, Loss: 0.18628934118896723\n",
            "Epoch 14/100, Loss: 0.3311542687006295\n",
            "Epoch 15/100, Loss: 0.1891329495701939\n",
            "Epoch 16/100, Loss: 0.4863142156973481\n",
            "Epoch 17/100, Loss: 0.35676152631640434\n",
            "Epoch 18/100, Loss: 0.1824147398583591\n",
            "Epoch 19/100, Loss: 0.10631818417459726\n",
            "Epoch 20/100, Loss: 0.05088087031617761\n",
            "Epoch 21/100, Loss: 0.041364516073372215\n",
            "Epoch 22/100, Loss: 0.03305039164843038\n",
            "Epoch 23/100, Loss: 0.2510677566751838\n",
            "Epoch 24/100, Loss: 0.32998351426795125\n",
            "Epoch 25/100, Loss: 0.13220992730930448\n",
            "Epoch 26/100, Loss: 0.31989247212186456\n",
            "Epoch 27/100, Loss: 0.14245118480175734\n",
            "Epoch 28/100, Loss: 0.09224083553999662\n",
            "Epoch 29/100, Loss: 0.06737226020777598\n",
            "Epoch 30/100, Loss: 0.04942837660200894\n",
            "Epoch 31/100, Loss: 0.12853941338835284\n",
            "Epoch 32/100, Loss: 0.12822710897307843\n",
            "Epoch 33/100, Loss: 0.06426946865394711\n",
            "Epoch 34/100, Loss: 0.16248865728266537\n",
            "Epoch 35/100, Loss: 0.06586171151138842\n",
            "Epoch 36/100, Loss: 0.23680315562523901\n",
            "Epoch 37/100, Loss: 0.4909311558585614\n",
            "Epoch 38/100, Loss: 0.3127411128953099\n",
            "Epoch 39/100, Loss: 0.3141096532344818\n",
            "Epoch 40/100, Loss: 0.28474041214212775\n",
            "Epoch 41/100, Loss: 0.1701751323416829\n",
            "Epoch 42/100, Loss: 0.22422465309500694\n",
            "Epoch 43/100, Loss: 0.22966507403180003\n",
            "Epoch 44/100, Loss: 0.15480282390490174\n",
            "Epoch 45/100, Loss: 0.140155129134655\n",
            "Epoch 46/100, Loss: 0.06898675968113821\n",
            "Epoch 47/100, Loss: 0.05080292536877096\n",
            "Epoch 48/100, Loss: 0.04728801269084215\n",
            "Epoch 49/100, Loss: 0.07476504231453873\n",
            "Epoch 50/100, Loss: 0.014383023866685107\n",
            "Epoch 51/100, Loss: 0.046242952812463045\n",
            "Epoch 52/100, Loss: 0.018939545581815764\n",
            "Epoch 53/100, Loss: 0.41157790532452054\n",
            "Epoch 54/100, Loss: 0.13720435136929154\n",
            "Epoch 55/100, Loss: 0.156775183044374\n",
            "Epoch 56/100, Loss: 0.26827310770750046\n",
            "Epoch 57/100, Loss: 0.09100918751209974\n",
            "Epoch 58/100, Loss: 0.20973120210692286\n",
            "Epoch 59/100, Loss: 0.09244808892253786\n",
            "Epoch 60/100, Loss: 0.11038676416501403\n",
            "Epoch 61/100, Loss: 0.06509822595398873\n",
            "Epoch 62/100, Loss: 0.039022444863803685\n",
            "Epoch 63/100, Loss: 0.021473694141604938\n",
            "Epoch 64/100, Loss: 0.01769296283600852\n",
            "Epoch 65/100, Loss: 0.6277620808250504\n",
            "Epoch 66/100, Loss: 0.19986286433413625\n",
            "Epoch 67/100, Loss: 0.3871563086286187\n",
            "Epoch 68/100, Loss: 0.1613623418379575\n",
            "Epoch 69/100, Loss: 0.09206127689685673\n",
            "Epoch 70/100, Loss: 0.04847493057604879\n",
            "Epoch 71/100, Loss: 0.06153279135469347\n",
            "Epoch 72/100, Loss: 0.019715030619408935\n",
            "Epoch 73/100, Loss: 0.016872542502824217\n",
            "Epoch 74/100, Loss: 0.01727249936084263\n",
            "Epoch 75/100, Loss: 0.012045421015500324\n",
            "Epoch 76/100, Loss: 0.012495269082137384\n",
            "Epoch 77/100, Loss: 0.010411029485112522\n",
            "Epoch 78/100, Loss: 0.01166840586665785\n",
            "Epoch 79/100, Loss: 0.4847067674563732\n",
            "Epoch 80/100, Loss: 0.03099695555283688\n",
            "Epoch 81/100, Loss: 0.08968233270570636\n",
            "Epoch 82/100, Loss: 0.07396985142258927\n",
            "Epoch 83/100, Loss: 0.2949476677531493\n",
            "Epoch 84/100, Loss: 0.1996906768763438\n",
            "Epoch 85/100, Loss: 0.4078687778674066\n",
            "Epoch 86/100, Loss: 0.10518132732249796\n",
            "Epoch 87/100, Loss: 0.04168530972674489\n",
            "Epoch 88/100, Loss: 0.08450538851320744\n",
            "Epoch 89/100, Loss: 0.0522491461597383\n",
            "Epoch 90/100, Loss: 0.015585771296173334\n",
            "Epoch 91/100, Loss: 0.10300616739550605\n",
            "Epoch 92/100, Loss: 0.12629264505812898\n",
            "Epoch 93/100, Loss: 0.0571212315117009\n",
            "Epoch 94/100, Loss: 0.05950423877220601\n",
            "Epoch 95/100, Loss: 0.25019376556156203\n",
            "Epoch 96/100, Loss: 0.21693397860508412\n",
            "Epoch 97/100, Loss: 0.30707887560129166\n",
            "Epoch 98/100, Loss: 0.11754606291651726\n",
            "Epoch 99/100, Loss: 0.06408048036973923\n",
            "Epoch 100/100, Loss: 0.07361661223694682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5cf2efc"
      },
      "source": [
        "**Reasoning**:\n",
        "The model has been trained for more epochs. Now, evaluate the model's performance on the validation set again to see if the accuracy has improved and calculate the metrics as before.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "788b8e2f",
        "outputId": "f40be48f-337a-45e1-cc82-d1d67fc963bd"
      },
      "source": [
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in valid_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        y_true.extend(labels.numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Sensitivity (Recall) & Specificity (per class)\n",
        "report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "for cls in class_names:\n",
        "    print(f\"{cls} - Sensitivity (Recall): {report[cls]['recall']:.4f}, Specificity: TBD\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.6786\n",
            "Mild - Sensitivity (Recall): 0.2500, Specificity: TBD\n",
            "Normal - Sensitivity (Recall): 0.8500, Specificity: TBD\n",
            "moderate - Sensitivity (Recall): 0.3333, Specificity: TBD\n",
            "severe - Sensitivity (Recall): 0.0000, Specificity: TBD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ade25d4"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate and print the specificity for each class using the confusion matrix and the classification report.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLbZTd12ZtS4",
        "outputId": "7af1d9b2-571b-4c54-91be-a8bf3b5c18af"
      },
      "source": [
        "for i, cls in enumerate(class_names):\n",
        "    # Calculate Specificity for each class\n",
        "    # Specificity for class i = TN / (TN + FP)\n",
        "    # TP: True Positives (diagonal element cm[i, i])\n",
        "    # FN: False Negatives (sum of row i excluding diagonal)\n",
        "    # FP: False Positives (sum of column i excluding diagonal)\n",
        "    # TN: True Negatives (sum of all elements excluding row i and column i)\n",
        "\n",
        "    TP = cm[i, i]\n",
        "    FN = np.sum(cm[i, :]) - TP\n",
        "    FP = np.sum(cm[:, i]) - TP\n",
        "    TN = np.sum(cm) - TP - FN - FP\n",
        "\n",
        "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
        "\n",
        "    print(f\"{cls} - Sensitivity (Recall): {report[cls]['recall']:.4f}, Specificity: {specificity:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mild - Sensitivity (Recall): 0.2500, Specificity: 0.8333\n",
            "Normal - Sensitivity (Recall): 0.8500, Specificity: 0.5000\n",
            "moderate - Sensitivity (Recall): 0.3333, Specificity: 0.9600\n",
            "severe - Sensitivity (Recall): 0.0000, Specificity: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "061ed4e1"
      },
      "source": [
        "## Implement data augmentation\n",
        "\n",
        "### Subtask:\n",
        "Apply data augmentation techniques to increase the size and variability of the training dataset, which can help the model generalize better.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e4593ec"
      },
      "source": [
        "**Reasoning**:\n",
        "Apply data augmentation to the training data by defining new transforms and updating the training dataset and dataloader.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58a23810"
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Transforms with augmentation for training data\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Transforms for validation data (no augmentation)\n",
        "valid_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Update Datasets with new transforms\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "valid_dataset = datasets.ImageFolder(valid_dir, transform=valid_transform)\n",
        "\n",
        "# Re-create Dataloader for training data\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06a790cb"
      },
      "source": [
        "## Fine-tune the learning rate\n",
        "\n",
        "### Subtask:\n",
        "Experiment with different learning rates to find one that allows the model to converge more effectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd881378"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a list of learning rates and iterate through them, training and evaluating the model for each to find the best one.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2391a145",
        "outputId": "c80edcde-8075-4a78-a13d-9575d495d360"
      },
      "source": [
        "learning_rates = [0.01, 0.001, 0.0001]\n",
        "best_accuracy = 0\n",
        "best_lr = None\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"Training with learning rate: {lr}\")\n",
        "\n",
        "    # Redefine optimizer with current learning rate\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Train the model for a fixed number of epochs\n",
        "    num_epochs_experiment = 20  # Fixed number of epochs for the experiment\n",
        "    for epoch in range(num_epochs_experiment):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        print(f\"  Epoch {epoch+1}/{num_epochs_experiment}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f\"Accuracy with learning rate {lr}: {accuracy:.4f}\\n\")\n",
        "\n",
        "    # Keep track of the best learning rate\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_lr = lr\n",
        "\n",
        "print(f\"Best learning rate found: {best_lr} with accuracy: {best_accuracy:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with learning rate: 0.01\n",
            "  Epoch 1/20, Loss: 2.423934794962406\n",
            "  Epoch 2/20, Loss: 1.462270051240921\n",
            "  Epoch 3/20, Loss: 1.3457897156476974\n",
            "  Epoch 4/20, Loss: 1.288005106151104\n",
            "  Epoch 5/20, Loss: 1.4155889824032784\n",
            "  Epoch 6/20, Loss: 1.2819211706519127\n",
            "  Epoch 7/20, Loss: 1.1876063644886017\n",
            "  Epoch 8/20, Loss: 1.4824185445904732\n",
            "  Epoch 9/20, Loss: 1.2802995890378952\n",
            "  Epoch 10/20, Loss: 1.1602602005004883\n",
            "  Epoch 11/20, Loss: 1.2141933739185333\n",
            "  Epoch 12/20, Loss: 1.1198775395751\n",
            "  Epoch 13/20, Loss: 1.136034019291401\n",
            "  Epoch 14/20, Loss: 1.0897700265049934\n",
            "  Epoch 15/20, Loss: 1.282431498169899\n",
            "  Epoch 16/20, Loss: 1.3455937877297401\n",
            "  Epoch 17/20, Loss: 1.1318335011601448\n",
            "  Epoch 18/20, Loss: 1.116824246942997\n",
            "  Epoch 19/20, Loss: 1.1263310983777046\n",
            "  Epoch 20/20, Loss: 1.1300238221883774\n",
            "Accuracy with learning rate 0.01: 0.7143\n",
            "\n",
            "Training with learning rate: 0.001\n",
            "  Epoch 1/20, Loss: 1.0072174817323685\n",
            "  Epoch 2/20, Loss: 1.1385728791356087\n",
            "  Epoch 3/20, Loss: 1.087582379579544\n",
            "  Epoch 4/20, Loss: 0.9845498651266098\n",
            "  Epoch 5/20, Loss: 0.9903450682759285\n",
            "  Epoch 6/20, Loss: 0.9653518795967102\n",
            "  Epoch 7/20, Loss: 0.9811095111072063\n",
            "  Epoch 8/20, Loss: 0.9768846854567528\n",
            "  Epoch 9/20, Loss: 1.061927244067192\n",
            "  Epoch 10/20, Loss: 1.0799631401896477\n",
            "  Epoch 11/20, Loss: 1.048567995429039\n",
            "  Epoch 12/20, Loss: 0.9876561760902405\n",
            "  Epoch 13/20, Loss: 1.0422236770391464\n",
            "  Epoch 14/20, Loss: 0.9449513778090477\n",
            "  Epoch 15/20, Loss: 1.008228711783886\n",
            "  Epoch 16/20, Loss: 1.015788123011589\n",
            "  Epoch 17/20, Loss: 0.9099908024072647\n",
            "  Epoch 18/20, Loss: 0.8978975638747215\n",
            "  Epoch 19/20, Loss: 1.079231895506382\n",
            "  Epoch 20/20, Loss: 0.9057637378573418\n",
            "Accuracy with learning rate 0.001: 0.7500\n",
            "\n",
            "Training with learning rate: 0.0001\n",
            "  Epoch 1/20, Loss: 0.9275489896535873\n",
            "  Epoch 2/20, Loss: 1.0184595733880997\n",
            "  Epoch 3/20, Loss: 0.8971236050128937\n",
            "  Epoch 4/20, Loss: 0.9011131376028061\n",
            "  Epoch 5/20, Loss: 0.866484060883522\n",
            "  Epoch 6/20, Loss: 0.8625849187374115\n",
            "  Epoch 7/20, Loss: 0.8863588646054268\n",
            "  Epoch 8/20, Loss: 0.8601971864700317\n",
            "  Epoch 9/20, Loss: 0.8946958780288696\n",
            "  Epoch 10/20, Loss: 0.8271774873137474\n",
            "  Epoch 11/20, Loss: 0.8246698565781116\n",
            "  Epoch 12/20, Loss: 0.8448927700519562\n",
            "  Epoch 13/20, Loss: 0.8512261211872101\n",
            "  Epoch 14/20, Loss: 0.8357046768069267\n",
            "  Epoch 15/20, Loss: 0.8694545105099678\n",
            "  Epoch 16/20, Loss: 0.822118766605854\n",
            "  Epoch 17/20, Loss: 0.7974893376231194\n",
            "  Epoch 18/20, Loss: 1.063570000231266\n",
            "  Epoch 19/20, Loss: 0.8003453016281128\n",
            "  Epoch 20/20, Loss: 0.7788051441311836\n",
            "Accuracy with learning rate 0.0001: 0.6429\n",
            "\n",
            "Best learning rate found: 0.001 with accuracy: 0.7500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6c0be14"
      },
      "source": [
        "## Use a different optimizer\n",
        "\n",
        "### Subtask:\n",
        "Try using a different optimizer, such as SGD with momentum or RMSprop, which might be better suited for this task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a6d1d0a"
      },
      "source": [
        "**Reasoning**:\n",
        "Define and iterate through different optimizers, training and evaluating the model with each to find the best one based on accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dbfc8f9",
        "outputId": "18583930-3d62-496c-ca29-5d8fdb776498"
      },
      "source": [
        "optimizers_to_experiment = {\n",
        "    \"SGD_momentum\": optim.SGD(model.parameters(), lr=best_lr, momentum=0.9),\n",
        "    \"RMSprop\": optim.RMSprop(model.parameters(), lr=best_lr),\n",
        "    \"Adam\": optim.Adam(model.parameters(), lr=best_lr) # Include Adam for comparison\n",
        "}\n",
        "\n",
        "best_optimizer_name = None\n",
        "best_optimizer_accuracy = best_accuracy # Initialize with the best accuracy from the previous step\n",
        "\n",
        "for optimizer_name, optimizer in optimizers_to_experiment.items():\n",
        "    print(f\"Training with optimizer: {optimizer_name}\")\n",
        "\n",
        "    # Train the model for a fixed number of epochs\n",
        "    for epoch in range(num_epochs_experiment):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        print(f\"  Epoch {epoch+1}/{num_epochs_experiment}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f\"Accuracy with {optimizer_name}: {accuracy:.4f}\\n\")\n",
        "\n",
        "    # Keep track of the best optimizer\n",
        "    if accuracy > best_optimizer_accuracy:\n",
        "        best_optimizer_accuracy = accuracy\n",
        "        best_optimizer_name = optimizer_name\n",
        "\n",
        "print(f\"Best optimizer found: {best_optimizer_name} with accuracy: {best_optimizer_accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with optimizer: SGD_momentum\n",
            "  Epoch 1/20, Loss: 0.9382601007819176\n",
            "  Epoch 2/20, Loss: 0.8806280195713043\n",
            "  Epoch 3/20, Loss: 0.8828477337956429\n",
            "  Epoch 4/20, Loss: 0.8004813268780708\n",
            "  Epoch 5/20, Loss: 0.7817757576704025\n",
            "  Epoch 6/20, Loss: 0.9125477746129036\n",
            "  Epoch 7/20, Loss: 0.9550539329648018\n",
            "  Epoch 8/20, Loss: 0.8654084205627441\n",
            "  Epoch 9/20, Loss: 0.8151852414011955\n",
            "  Epoch 10/20, Loss: 0.8707642108201981\n",
            "  Epoch 11/20, Loss: 0.8234695717692375\n",
            "  Epoch 12/20, Loss: 0.8162484541535378\n",
            "  Epoch 13/20, Loss: 0.8198707103729248\n",
            "  Epoch 14/20, Loss: 0.8612731471657753\n",
            "  Epoch 15/20, Loss: 0.8850715458393097\n",
            "  Epoch 16/20, Loss: 0.8292203322052956\n",
            "  Epoch 17/20, Loss: 0.9766038581728935\n",
            "  Epoch 18/20, Loss: 0.847256176173687\n",
            "  Epoch 19/20, Loss: 0.844158947467804\n",
            "  Epoch 20/20, Loss: 0.8270358219742775\n",
            "Accuracy with SGD_momentum: 0.6786\n",
            "\n",
            "Training with optimizer: RMSprop\n",
            "  Epoch 1/20, Loss: 0.9763980358839035\n",
            "  Epoch 2/20, Loss: 1.08404640853405\n",
            "  Epoch 3/20, Loss: 0.9744727313518524\n",
            "  Epoch 4/20, Loss: 0.9165655374526978\n",
            "  Epoch 5/20, Loss: 0.8904668241739273\n",
            "  Epoch 6/20, Loss: 0.852725125849247\n",
            "  Epoch 7/20, Loss: 0.884915329515934\n",
            "  Epoch 8/20, Loss: 0.8125138133764267\n",
            "  Epoch 9/20, Loss: 0.9872370734810829\n",
            "  Epoch 10/20, Loss: 0.8341991230845451\n",
            "  Epoch 11/20, Loss: 0.9733068570494652\n",
            "  Epoch 12/20, Loss: 0.8618002831935883\n",
            "  Epoch 13/20, Loss: 0.8792455419898033\n",
            "  Epoch 14/20, Loss: 0.8592408448457718\n",
            "  Epoch 15/20, Loss: 1.0016654878854752\n",
            "  Epoch 16/20, Loss: 0.8525219485163689\n",
            "  Epoch 17/20, Loss: 1.0433294996619225\n",
            "  Epoch 18/20, Loss: 0.8461922481656075\n",
            "  Epoch 19/20, Loss: 0.7537911236286163\n",
            "  Epoch 20/20, Loss: 0.9239494949579239\n",
            "Accuracy with RMSprop: 0.7500\n",
            "\n",
            "Training with optimizer: Adam\n",
            "  Epoch 1/20, Loss: 0.8445951491594315\n",
            "  Epoch 2/20, Loss: 0.876818411052227\n",
            "  Epoch 3/20, Loss: 0.8320411145687103\n",
            "  Epoch 4/20, Loss: 0.8130332380533218\n",
            "  Epoch 5/20, Loss: 0.8025476858019829\n",
            "  Epoch 6/20, Loss: 0.8180724456906319\n",
            "  Epoch 7/20, Loss: 0.7040956541895866\n",
            "  Epoch 8/20, Loss: 0.7463082149624825\n",
            "  Epoch 9/20, Loss: 0.726345993578434\n",
            "  Epoch 10/20, Loss: 0.7093161530792713\n",
            "  Epoch 11/20, Loss: 0.7158232107758522\n",
            "  Epoch 12/20, Loss: 0.8141863271594048\n",
            "  Epoch 13/20, Loss: 0.8190077543258667\n",
            "  Epoch 14/20, Loss: 0.8025638684630394\n",
            "  Epoch 15/20, Loss: 1.0768543630838394\n",
            "  Epoch 16/20, Loss: 0.7452648319303989\n",
            "  Epoch 17/20, Loss: 0.8358956277370453\n",
            "  Epoch 18/20, Loss: 0.7236337512731552\n",
            "  Epoch 19/20, Loss: 0.8287585154175758\n",
            "  Epoch 20/20, Loss: 0.7780792340636253\n",
            "Accuracy with Adam: 0.7143\n",
            "\n",
            "Best optimizer found: None with accuracy: 0.7500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "375aba16"
      },
      "source": [
        "## Adjust model architecture\n",
        "\n",
        "### Subtask:\n",
        "Adjust the model architecture by using a more powerful pre-trained model to improve classification accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a5e77e6"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the ResNet-50 model with pre-trained weights, freeze its parameters, replace the final layer, and move it to the appropriate device as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5e0e25e",
        "outputId": "7b6ca351-30a2-491f-a816-a642df01cf18"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# Load ResNet-50\n",
        "resnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Freeze parameters\n",
        "for param in resnet50.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the final fully connected layer\n",
        "num_ftrs = resnet50.fc.in_features\n",
        "resnet50.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "\n",
        "# Move model to device\n",
        "model = resnet50.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 108MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6d78ff7"
      },
      "source": [
        "## Fine-tune the learning rate\n",
        "\n",
        "### Subtask:\n",
        "Experiment with different learning rates to find one that allows the model to converge more effectively with the new ResNet-50 architecture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b3a2755"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a list of learning rates, initialize variables for tracking the best accuracy and learning rate, and then iterate through the learning rates to train and evaluate the model, updating the best values found.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8f1dce9",
        "outputId": "0057b2c3-783b-47ca-fa02-3e74ebd0e517"
      },
      "source": [
        "learning_rates = [0.01, 0.001, 0.0001]\n",
        "best_accuracy = 0\n",
        "best_lr = None\n",
        "num_epochs_experiment = 20\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"Training with learning rate: {lr}\")\n",
        "\n",
        "    # Redefine optimizer with current learning rate (using Adam as it performed well)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Train the model for a fixed number of epochs\n",
        "    for epoch in range(num_epochs_experiment):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        print(f\"  Epoch {epoch+1}/{num_epochs_experiment}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f\"Accuracy with learning rate {lr}: {accuracy:.4f}\\n\")\n",
        "\n",
        "    # Keep track of the best learning rate\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_lr = lr\n",
        "\n",
        "print(f\"Best learning rate found: {best_lr} with accuracy: {best_accuracy:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with learning rate: 0.01\n",
            "  Epoch 1/20, Loss: 4.788638070225716\n",
            "  Epoch 2/20, Loss: 3.194862797856331\n",
            "  Epoch 3/20, Loss: 1.8934206068515778\n",
            "  Epoch 4/20, Loss: 1.4486229121685028\n",
            "  Epoch 5/20, Loss: 1.285699762403965\n",
            "  Epoch 6/20, Loss: 0.942675031721592\n",
            "  Epoch 7/20, Loss: 1.1941833719611168\n",
            "  Epoch 8/20, Loss: 1.3829333782196045\n",
            "  Epoch 9/20, Loss: 0.9839236587285995\n",
            "  Epoch 10/20, Loss: 1.1318470537662506\n",
            "  Epoch 11/20, Loss: 1.0379326865077019\n",
            "  Epoch 12/20, Loss: 0.8577755987644196\n",
            "  Epoch 13/20, Loss: 1.035595752298832\n",
            "  Epoch 14/20, Loss: 1.1653383672237396\n",
            "  Epoch 15/20, Loss: 1.009760294109583\n",
            "  Epoch 16/20, Loss: 0.9517651051282883\n",
            "  Epoch 17/20, Loss: 0.6601657047867775\n",
            "  Epoch 18/20, Loss: 1.0800646506249905\n",
            "  Epoch 19/20, Loss: 1.2210949137806892\n",
            "  Epoch 20/20, Loss: 1.0486162453889847\n",
            "Accuracy with learning rate 0.01: 0.5000\n",
            "\n",
            "Training with learning rate: 0.001\n",
            "  Epoch 1/20, Loss: 0.7541013769805431\n",
            "  Epoch 2/20, Loss: 0.6002805158495903\n",
            "  Epoch 3/20, Loss: 0.4598154090344906\n",
            "  Epoch 4/20, Loss: 0.44775775633752346\n",
            "  Epoch 5/20, Loss: 0.5095854476094246\n",
            "  Epoch 6/20, Loss: 0.5421052202582359\n",
            "  Epoch 7/20, Loss: 0.47981157526373863\n",
            "  Epoch 8/20, Loss: 0.5105563960969448\n",
            "  Epoch 9/20, Loss: 0.4791125189512968\n",
            "  Epoch 10/20, Loss: 0.519819937646389\n",
            "  Epoch 11/20, Loss: 0.4572804495692253\n",
            "  Epoch 12/20, Loss: 0.375532828271389\n",
            "  Epoch 13/20, Loss: 0.36320151574909687\n",
            "  Epoch 14/20, Loss: 0.3235060013830662\n",
            "  Epoch 15/20, Loss: 0.4426572825759649\n",
            "  Epoch 16/20, Loss: 0.5905824452638626\n",
            "  Epoch 17/20, Loss: 0.45720680244266987\n",
            "  Epoch 18/20, Loss: 0.4558587484061718\n",
            "  Epoch 19/20, Loss: 0.3979921881109476\n",
            "  Epoch 20/20, Loss: 0.4347172863781452\n",
            "Accuracy with learning rate 0.001: 0.8214\n",
            "\n",
            "Training with learning rate: 0.0001\n",
            "  Epoch 1/20, Loss: 0.33852518163621426\n",
            "  Epoch 2/20, Loss: 0.3394726309925318\n",
            "  Epoch 3/20, Loss: 0.43721067160367966\n",
            "  Epoch 4/20, Loss: 0.32389897108078003\n",
            "  Epoch 5/20, Loss: 0.4177405871450901\n",
            "  Epoch 6/20, Loss: 0.29963170923292637\n",
            "  Epoch 7/20, Loss: 0.38185469061136246\n",
            "  Epoch 8/20, Loss: 0.39343277737498283\n",
            "  Epoch 9/20, Loss: 0.3962020240724087\n",
            "  Epoch 10/20, Loss: 0.25422544591128826\n",
            "  Epoch 11/20, Loss: 0.41298313811421394\n",
            "  Epoch 12/20, Loss: 0.35409408807754517\n",
            "  Epoch 13/20, Loss: 0.49776821583509445\n",
            "  Epoch 14/20, Loss: 0.42633328400552273\n",
            "  Epoch 15/20, Loss: 0.35465096961706877\n",
            "  Epoch 16/20, Loss: 0.2846469823271036\n",
            "  Epoch 17/20, Loss: 0.3439759239554405\n",
            "  Epoch 18/20, Loss: 0.34342942014336586\n",
            "  Epoch 19/20, Loss: 0.37307089008390903\n",
            "  Epoch 20/20, Loss: 0.40671043656766415\n",
            "Accuracy with learning rate 0.0001: 0.8214\n",
            "\n",
            "Best learning rate found: 0.001 with accuracy: 0.8214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82e7371c"
      },
      "source": [
        "## Use a different optimizer\n",
        "\n",
        "### Subtask:\n",
        "Try using a different optimizer, such as SGD with momentum or RMSprop, which might be better suited for this task with the ResNet-50 architecture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4848851"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the optimizers to experiment with, initialize variables to track the best optimizer, and then iterate through the optimizers, train the model for a fixed number of epochs, evaluate the accuracy, and update the best optimizer if the current one performs better.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d35afa83",
        "outputId": "f92a746f-81c3-4093-bfd6-2fcc772d6b94"
      },
      "source": [
        "optimizers_to_experiment = {\n",
        "    \"SGD_momentum\": optim.SGD(model.parameters(), lr=best_lr, momentum=0.9),\n",
        "    \"RMSprop\": optim.RMSprop(model.parameters(), lr=best_lr),\n",
        "    \"Adam\": optim.Adam(model.parameters(), lr=best_lr) # Include Adam for comparison\n",
        "}\n",
        "\n",
        "best_optimizer_name = None\n",
        "best_optimizer_accuracy = best_accuracy # Initialize with the best accuracy from the previous step\n",
        "num_epochs_experiment = 20 # Use the same number of epochs as in the learning rate tuning\n",
        "\n",
        "for optimizer_name, optimizer in optimizers_to_experiment.items():\n",
        "    print(f\"Training with optimizer: {optimizer_name}\")\n",
        "\n",
        "    # Train the model for a fixed number of epochs\n",
        "    for epoch in range(num_epochs_experiment):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        print(f\"  Epoch {epoch+1}/{num_epochs_experiment}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f\"Accuracy with {optimizer_name}: {accuracy:.4f}\\n\")\n",
        "\n",
        "    # Keep track of the best optimizer\n",
        "    if accuracy > best_optimizer_accuracy:\n",
        "        best_optimizer_accuracy = accuracy\n",
        "        best_optimizer_name = optimizer_name\n",
        "\n",
        "print(f\"Best optimizer found: {best_optimizer_name} with accuracy: {best_optimizer_accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with optimizer: SGD_momentum\n",
            "  Epoch 1/20, Loss: 0.5303780511021614\n",
            "  Epoch 2/20, Loss: 0.44337386824190617\n",
            "  Epoch 3/20, Loss: 0.3674008287489414\n",
            "  Epoch 4/20, Loss: 0.3870375193655491\n",
            "  Epoch 5/20, Loss: 0.3291462976485491\n",
            "  Epoch 6/20, Loss: 0.41386925615370274\n",
            "  Epoch 7/20, Loss: 0.44887036085128784\n",
            "  Epoch 8/20, Loss: 0.46327805891633034\n",
            "  Epoch 9/20, Loss: 0.4251550640910864\n",
            "  Epoch 10/20, Loss: 0.36350465193390846\n",
            "  Epoch 11/20, Loss: 0.41404012218117714\n",
            "  Epoch 12/20, Loss: 0.34882502630352974\n",
            "  Epoch 13/20, Loss: 0.33676775358617306\n",
            "  Epoch 14/20, Loss: 0.3414156176149845\n",
            "  Epoch 15/20, Loss: 0.3836707826703787\n",
            "  Epoch 16/20, Loss: 0.36359394900500774\n",
            "  Epoch 17/20, Loss: 0.5164346769452095\n",
            "  Epoch 18/20, Loss: 0.42734207957983017\n",
            "  Epoch 19/20, Loss: 0.43782669492065907\n",
            "  Epoch 20/20, Loss: 0.36716214194893837\n",
            "Accuracy with SGD_momentum: 0.8214\n",
            "\n",
            "Training with optimizer: RMSprop\n",
            "  Epoch 1/20, Loss: 1.858758706599474\n",
            "  Epoch 2/20, Loss: 0.7283057197928429\n",
            "  Epoch 3/20, Loss: 0.6085481382906437\n",
            "  Epoch 4/20, Loss: 0.7005947418510914\n",
            "  Epoch 5/20, Loss: 0.45914389193058014\n",
            "  Epoch 6/20, Loss: 0.7044472694396973\n",
            "  Epoch 7/20, Loss: 0.5153152458369732\n",
            "  Epoch 8/20, Loss: 0.3759908899664879\n",
            "  Epoch 9/20, Loss: 0.4761651437729597\n",
            "  Epoch 10/20, Loss: 0.39436989091336727\n",
            "  Epoch 11/20, Loss: 0.477592246606946\n",
            "  Epoch 12/20, Loss: 0.4144955798983574\n",
            "  Epoch 13/20, Loss: 0.3394198901951313\n",
            "  Epoch 14/20, Loss: 0.36047534737735987\n",
            "  Epoch 15/20, Loss: 0.41252026706933975\n",
            "  Epoch 16/20, Loss: 0.3292079996317625\n",
            "  Epoch 17/20, Loss: 0.506463224068284\n",
            "  Epoch 18/20, Loss: 0.6628673281520605\n",
            "  Epoch 19/20, Loss: 0.4892894309014082\n",
            "  Epoch 20/20, Loss: 0.36928340792655945\n",
            "Accuracy with RMSprop: 0.7857\n",
            "\n",
            "Training with optimizer: Adam\n",
            "  Epoch 1/20, Loss: 0.37095299176871777\n",
            "  Epoch 2/20, Loss: 0.3421783298254013\n",
            "  Epoch 3/20, Loss: 0.48304309509694576\n",
            "  Epoch 4/20, Loss: 0.36969295144081116\n",
            "  Epoch 5/20, Loss: 0.3362152948975563\n",
            "  Epoch 6/20, Loss: 0.36380262672901154\n",
            "  Epoch 7/20, Loss: 0.462994784116745\n",
            "  Epoch 8/20, Loss: 0.38002606574445963\n",
            "  Epoch 9/20, Loss: 0.36209378205239773\n",
            "  Epoch 10/20, Loss: 0.31661195307970047\n",
            "  Epoch 11/20, Loss: 0.36788068898022175\n",
            "  Epoch 12/20, Loss: 0.3429709952324629\n",
            "  Epoch 13/20, Loss: 0.21264194697141647\n",
            "  Epoch 14/20, Loss: 0.4021654464304447\n",
            "  Epoch 15/20, Loss: 0.35586414486169815\n",
            "  Epoch 16/20, Loss: 0.35948812775313854\n",
            "  Epoch 17/20, Loss: 0.3479835018515587\n",
            "  Epoch 18/20, Loss: 0.32690922915935516\n",
            "  Epoch 19/20, Loss: 0.3329587634652853\n",
            "  Epoch 20/20, Loss: 0.35797272250056267\n",
            "Accuracy with Adam: 0.8571\n",
            "\n",
            "Best optimizer found: Adam with accuracy: 0.8571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e468af0a"
      },
      "source": [
        "## Evaluate with more metrics\n",
        "\n",
        "### Subtask:\n",
        "Calculate and analyze additional evaluation metrics like precision, F1-score, and per-class accuracy to get a more comprehensive understanding of the model's performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edd10761"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate and print the classification report to get a detailed breakdown of precision, recall, F1-score, and support for each class, which is necessary to understand the model's performance beyond overall accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d843615b",
        "outputId": "285d1103-50c9-407f-8055-88f2a075a1f3"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Calculate and print the classification report\n",
        "report = classification_report(y_true, y_pred, target_names=class_names)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n",
        "\n",
        "# Analyze the report (this will be done manually based on the printed output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Mild       0.50      0.50      0.50         4\n",
            "      Normal       0.91      1.00      0.95        20\n",
            "    moderate       1.00      0.67      0.80         3\n",
            "      severe       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.86        28\n",
            "   macro avg       0.60      0.54      0.56        28\n",
            "weighted avg       0.83      0.86      0.84        28\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d4195da"
      },
      "source": [
        "## Increase epochs\n",
        "\n",
        "### Subtask:\n",
        "Train the model for more epochs to allow it to learn more from the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5cb763c"
      },
      "source": [
        "**Reasoning**:\n",
        "Train the model for more epochs using the best optimizer and learning rate found in previous steps to see if it improves accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b74c84eb",
        "outputId": "2998b658-a628-4c31-d67f-f659e4b82a54"
      },
      "source": [
        "# Use the best optimizer and learning rate from previous experiments\n",
        "optimizer = optimizers_to_experiment[\"Adam\"] # Based on previous experiment results\n",
        "optimizer.lr = best_lr # Use the best learning rate found\n",
        "\n",
        "num_epochs = 100 # Increased for more training\n",
        "\n",
        "print(f\"Training with Adam optimizer and learning rate: {best_lr} for {num_epochs} epochs.\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Adam optimizer and learning rate: 0.001 for 100 epochs.\n",
            "Epoch 1/100, Loss: 0.5038147885352373\n",
            "Epoch 2/100, Loss: 0.4334309007972479\n",
            "Epoch 3/100, Loss: 0.31455086916685104\n",
            "Epoch 4/100, Loss: 0.33859183453023434\n",
            "Epoch 5/100, Loss: 0.3904418386518955\n",
            "Epoch 6/100, Loss: 0.36577415838837624\n",
            "Epoch 7/100, Loss: 0.3941432163119316\n",
            "Epoch 8/100, Loss: 0.439587002620101\n",
            "Epoch 9/100, Loss: 0.4401077330112457\n",
            "Epoch 10/100, Loss: 0.3772455733269453\n",
            "Epoch 11/100, Loss: 0.2954315263777971\n",
            "Epoch 12/100, Loss: 0.3748079650104046\n",
            "Epoch 13/100, Loss: 0.40573081001639366\n",
            "Epoch 14/100, Loss: 0.3924034219235182\n",
            "Epoch 15/100, Loss: 0.33317151479423046\n",
            "Epoch 16/100, Loss: 0.5978201851248741\n",
            "Epoch 17/100, Loss: 0.36167024821043015\n",
            "Epoch 18/100, Loss: 0.3788159843534231\n",
            "Epoch 19/100, Loss: 0.3119277935475111\n",
            "Epoch 20/100, Loss: 0.2934840973466635\n",
            "Epoch 21/100, Loss: 0.45480391569435596\n",
            "Epoch 22/100, Loss: 0.45656094141304493\n",
            "Epoch 23/100, Loss: 0.38080362789332867\n",
            "Epoch 24/100, Loss: 0.289045125246048\n",
            "Epoch 25/100, Loss: 0.31267167441546917\n",
            "Epoch 26/100, Loss: 0.5213831439614296\n",
            "Epoch 27/100, Loss: 0.5426509194076061\n",
            "Epoch 28/100, Loss: 0.42632343992590904\n",
            "Epoch 29/100, Loss: 0.4108836278319359\n",
            "Epoch 30/100, Loss: 0.3989250548183918\n",
            "Epoch 31/100, Loss: 0.27016589138656855\n",
            "Epoch 32/100, Loss: 0.3158912155777216\n",
            "Epoch 33/100, Loss: 0.413624070584774\n",
            "Epoch 34/100, Loss: 0.37914945371448994\n",
            "Epoch 35/100, Loss: 0.36405118741095066\n",
            "Epoch 36/100, Loss: 0.2993727829307318\n",
            "Epoch 37/100, Loss: 0.47875761426985264\n",
            "Epoch 38/100, Loss: 0.5618858970701694\n",
            "Epoch 39/100, Loss: 0.38820343650877476\n",
            "Epoch 40/100, Loss: 0.38979208283126354\n",
            "Epoch 41/100, Loss: 0.42895832285284996\n",
            "Epoch 42/100, Loss: 0.6763299070298672\n",
            "Epoch 43/100, Loss: 0.35009085945785046\n",
            "Epoch 44/100, Loss: 0.3527698777616024\n",
            "Epoch 45/100, Loss: 0.3784196777269244\n",
            "Epoch 46/100, Loss: 0.32037163712084293\n",
            "Epoch 47/100, Loss: 0.36265671998262405\n",
            "Epoch 48/100, Loss: 0.5232657287269831\n",
            "Epoch 49/100, Loss: 0.4765222817659378\n",
            "Epoch 50/100, Loss: 0.4577085115015507\n",
            "Epoch 51/100, Loss: 0.4333621487021446\n",
            "Epoch 52/100, Loss: 0.39564150385558605\n",
            "Epoch 53/100, Loss: 0.31896063685417175\n",
            "Epoch 54/100, Loss: 0.32880125753581524\n",
            "Epoch 55/100, Loss: 0.3024786226451397\n",
            "Epoch 56/100, Loss: 0.6088237687945366\n",
            "Epoch 57/100, Loss: 0.32995401322841644\n",
            "Epoch 58/100, Loss: 0.3369581736624241\n",
            "Epoch 59/100, Loss: 0.23288674838840961\n",
            "Epoch 60/100, Loss: 0.28849324956536293\n",
            "Epoch 61/100, Loss: 0.5112810954451561\n",
            "Epoch 62/100, Loss: 0.35952580720186234\n",
            "Epoch 63/100, Loss: 0.4388129711151123\n",
            "Epoch 64/100, Loss: 0.3478095345199108\n",
            "Epoch 65/100, Loss: 0.26780163682997227\n",
            "Epoch 66/100, Loss: 0.2741813324391842\n",
            "Epoch 67/100, Loss: 0.2802243288606405\n",
            "Epoch 68/100, Loss: 0.5168920271098614\n",
            "Epoch 69/100, Loss: 0.319433756172657\n",
            "Epoch 70/100, Loss: 0.3659987263381481\n",
            "Epoch 71/100, Loss: 0.22141517978161573\n",
            "Epoch 72/100, Loss: 0.3462932789698243\n",
            "Epoch 73/100, Loss: 0.38237272948026657\n",
            "Epoch 74/100, Loss: 0.3223132425919175\n",
            "Epoch 75/100, Loss: 0.2659312579780817\n",
            "Epoch 76/100, Loss: 0.3030617740005255\n",
            "Epoch 77/100, Loss: 0.3866113107651472\n",
            "Epoch 78/100, Loss: 0.3093383368104696\n",
            "Epoch 79/100, Loss: 0.2675257381051779\n",
            "Epoch 80/100, Loss: 0.23185151256620884\n",
            "Epoch 81/100, Loss: 0.424221882596612\n",
            "Epoch 82/100, Loss: 0.38703545182943344\n",
            "Epoch 83/100, Loss: 0.3319219369441271\n",
            "Epoch 84/100, Loss: 0.3870201800018549\n",
            "Epoch 85/100, Loss: 0.23352742940187454\n",
            "Epoch 86/100, Loss: 0.28134823869913816\n",
            "Epoch 87/100, Loss: 0.3018606510013342\n",
            "Epoch 88/100, Loss: 0.30184439569711685\n",
            "Epoch 89/100, Loss: 0.3478904329240322\n",
            "Epoch 90/100, Loss: 0.38867124170064926\n",
            "Epoch 91/100, Loss: 0.252346396446228\n",
            "Epoch 92/100, Loss: 0.25070113223046064\n",
            "Epoch 93/100, Loss: 0.2612180560827255\n",
            "Epoch 94/100, Loss: 0.24781264923512936\n",
            "Epoch 95/100, Loss: 0.2704256437718868\n",
            "Epoch 96/100, Loss: 0.2780000399798155\n",
            "Epoch 97/100, Loss: 0.39949596114456654\n",
            "Epoch 98/100, Loss: 0.4005505759268999\n",
            "Epoch 99/100, Loss: 0.314335273578763\n",
            "Epoch 100/100, Loss: 0.2576157050207257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88aaa6a5"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the model after training for more epochs using the validation set and calculate the accuracy, confusion matrix, and classification report to assess performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca46daf0",
        "outputId": "ffca2dde-5c09-4dc7-b695-5cea4c00b624"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in valid_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        y_true.extend(labels.numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Classification Report\n",
        "report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "# Calculate and print per-class sensitivity and specificity\n",
        "for i, cls in enumerate(class_names):\n",
        "    TP = cm[i, i]\n",
        "    FN = np.sum(cm[i, :]) - TP\n",
        "    FP = np.sum(cm[:, i]) - TP\n",
        "    TN = np.sum(cm) - TP - FN - FP\n",
        "\n",
        "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
        "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
        "\n",
        "    print(f\"{cls} - Sensitivity (Recall): {sensitivity:.4f}, Specificity: {specificity:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.8571\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Mild       0.67      0.50      0.57         4\n",
            "      Normal       0.95      1.00      0.98        20\n",
            "    moderate       0.50      0.67      0.57         3\n",
            "      severe       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.86        28\n",
            "   macro avg       0.53      0.54      0.53        28\n",
            "weighted avg       0.83      0.86      0.84        28\n",
            "\n",
            "Mild - Sensitivity (Recall): 0.5000, Specificity: 0.9583\n",
            "Normal - Sensitivity (Recall): 1.0000, Specificity: 0.8750\n",
            "moderate - Sensitivity (Recall): 0.6667, Specificity: 0.9200\n",
            "severe - Sensitivity (Recall): 0.0000, Specificity: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e4eaeaa"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   After training for 100 epochs with the initial model architecture, the overall accuracy was 0.6786.\n",
        "*   Applying data augmentation techniques to the training data was successfully implemented.\n",
        "*   Experimenting with different learning rates on the initial model architecture showed that a learning rate of 0.001 yielded the highest validation accuracy of 0.7500 after 20 epochs.\n",
        "*   Testing different optimizers on the initial model architecture revealed that the RMSprop optimizer achieved the highest accuracy of 0.7500 after 20 epochs with the best learning rate.\n",
        "*   Adjusting the model architecture by using a pre-trained ResNet-50 model with frozen parameters was successfully implemented.\n",
        "*   Fine-tuning the learning rate for the ResNet-50 model showed that both 0.001 and 0.0001 learning rates achieved an accuracy of 0.8214 after 20 epochs, with 0.001 being identified as the best.\n",
        "*   Experimenting with different optimizers for the ResNet-50 model indicated that the Adam optimizer achieved the highest accuracy of 0.8571 after 20 epochs with the best learning rate.\n",
        "*   Evaluating the ResNet-50 model trained with the best configuration (Adam optimizer, learning rate 0.001) for 100 epochs resulted in a validation accuracy of 0.8571.\n",
        "*   The classification report for the final model showed an overall accuracy of 0.86. Per-class metrics varied significantly, with the 'Normal' class having high recall (1.00) and the 'severe' class having a sensitivity (recall) of 0.0000.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The model's performance on the 'severe' class is a major weakness, significantly impacting overall metrics like the macro average F1-score (0.56). Future efforts should focus on improving the model's ability to classify this class, potentially through techniques like oversampling the minority class, using focal loss, or exploring different model architectures more suited for imbalanced datasets.\n",
        "*   Despite implementing various techniques like increasing epochs, data augmentation, and using a pre-trained model with hyperparameter tuning, the target accuracy of 89.3% was not met. Further investigation into more advanced techniques such as transfer learning with unfreezing some layers of the pre-trained model, exploring different pre-trained architectures, or implementing more sophisticated data augmentation strategies could be beneficial.\n"
      ]
    }
  ]
}